{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing code with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Lightning\n",
    "#!pip install lightning\n",
    "#!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "\n",
    "train_ds = datasets.MNIST(root = \"./mnist\", train = True, transform = transforms.ToTensor(), download=True)\n",
    "# # transforms.ToTensor() converts the image into numbers and scales the values between 0 and 1\n",
    "test_ds = datasets.MNIST(root = \"./mnist\", train = False, transform = transforms.ToTensor(), download=True)\n",
    "\n",
    "# Split the training dataset into training and validation dataset\n",
    "torch.manual_seed(2056)\n",
    "train_ds, val_ds = random_split(train_ds, [55000, 5000])\n",
    "\n",
    "# Create dataloaders\n",
    "# Create training, validation, and test dataloaders to load data in batches for model training\n",
    "train_dl = DataLoader(\n",
    "    dataset = train_ds,\n",
    "    batch_size = 64,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    dataset = val_ds,\n",
    "    batch_size = 64,\n",
    "    shuffle = False\n",
    ")   \n",
    "\n",
    "test_dl = DataLoader(\n",
    "    dataset = test_ds,\n",
    "    batch_size = 64,\n",
    "    shuffle = False\n",
    ")  \n",
    "\n",
    "# Create a model\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.all_layers = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(num_features, 50),\n",
    "            nn.ReLU(),\n",
    "            # Second hidden layer\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            # Output layer\n",
    "            nn.Linear(25, num_classes)\n",
    "        )\n",
    "\n",
    "    # Foward the input through the model\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # Flatten all dimensions except batch\n",
    "        logits = self.all_layers(x)\n",
    "        return logits\n",
    "    \n",
    "# Utility function to compute accuracy\n",
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    correct, total_examples = 0.0, 0.0\n",
    "    \n",
    "    # Put the model in eval mode\n",
    "    model = model.eval()\n",
    "\n",
    "    for idx, (features, targets) in enumerate(dataloader):\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logits = model(features)\n",
    "        \n",
    "            # Get predictions from the model\n",
    "            predicted_labels = torch.argmax(logits, dim = 1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct += torch.sum((predicted_labels == targets).float())\n",
    "\n",
    "            # Count the total number of examples\n",
    "            total_examples += len(targets)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = correct/total_examples\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LightningModule organizes your PyTorch code into 6 sections:\n",
    "\n",
    "`Initialization (__init__ and setup())`.\n",
    "\n",
    "`Train Loop (training_step())`\n",
    "\n",
    "`Validation Loop (validation_step())`\n",
    "\n",
    "`Test Loop (test_step())`\n",
    "\n",
    "`Prediction Loop (predict_step())`\n",
    "\n",
    "`Optimizers and LR Schedulers (configure_optimizers())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "# Define a LightningModule that receives Pytorch model as input\n",
    "class LightningModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Set up metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use forward pass of Pytorch model\n",
    "        return self.model(x)\n",
    "    \n",
    "    # Train loop\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        # Log metrics average loss across the epoch\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "\n",
    "        # Compute training accuracy after every epoch\n",
    "        predicted_labels = torch.argmax(logits, dim = 1)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar = True, on_epoch = True, on_step = False)\n",
    "\n",
    "\n",
    "        return loss # # Return the loss which will be passed to the optimizer to zero out gradients, compute gradients, and update weights\n",
    "    \n",
    "    # Validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        # Log metrics average loss across the epoch\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Compute validation accuracy after every epoch\n",
    "        predicted_labels = torch.argmax(logits, dim = 1)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar = True) # Doesn't need to specify on_epoch = True because it is the default behavior of validation_step\n",
    "        # Doesn't return anything because we don't need to compute gradients during validation\n",
    "\n",
    "    # Define optimizers and learning rate schedulers\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr = self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a test loop\n",
    "class LightningModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Set up metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use forward pass of Pytorch model\n",
    "        return self.model(x)\n",
    "    \n",
    "    # Define shared step for training, validation, and test steps\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        predicted_labels = torch.argmax(logits, dim = 1)\n",
    "        return loss, predicted_labels, true_labels\n",
    "    \n",
    "    # Train loop\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\",\n",
    "            self.train_acc,\n",
    "            prog_bar = True,\n",
    "            on_epoch = True, on_step = False\n",
    "        )\n",
    "        return loss # # Return the loss which will be passed to the optimizer to zero out gradients, compute gradients, and update weights\n",
    "    \n",
    "    # Validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar = True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar = True)\n",
    "\n",
    "    # Test loop\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    # Define optimizers and learning rate schedulers\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr = self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINER\n",
    "Once you’ve organized your PyTorch code into a LightningModule, the Trainer automates everything else.\n",
    "\n",
    "The Trainer achieves the following:\n",
    "\n",
    ">You maintain control over all aspects via PyTorch code in your LightningModule.\n",
    "\n",
    ">The trainer uses best practices embedded by contributors and users from top AI labs such as Facebook AI Research, NYU, MIT, Stanford, etc…\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch model\n",
    "pytorch_model = PyTorchMLP(num_features = 28*28, num_classes = 10)\n",
    "\n",
    "# Define a Lightning model\n",
    "lightning_model = LightningModel(model = pytorch_model, learning_rate = 0.05)\n",
    "\n",
    "# Customize aspects of the training process using a Trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs = 10,\n",
    "    accelerator = \"auto\", # Set to auto to use GPU if available\n",
    "    devices = \"auto\", # Set to auto to use all GPUs if available\n",
    "    #deterministic=True # Set to True to ensure reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(\n",
    "    model = lightning_model,\n",
    "    train_dataloaders = train_dl,\n",
    "    val_dataloaders = val_dl\n",
    ")\n",
    "\n",
    "# # Compute accuracy on test dataset\n",
    "# train_acc = compute_accuracy(pytorch_model, train_dl)\n",
    "# val_acc = compute_accuracy(pytorch_model, val_dl)\n",
    "# test_acc = compute_accuracy(pytorch_model, test_dl)\n",
    "# print(f\"\\nTrain accuracy: {train_acc*100:.2f}, \\nValidation accuracy: {val_acc*100:.2f}, \\nTest accuracy: {test_acc*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\dlf\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py:149: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\lightning_logs\\version_3\\checkpoints\\epoch=9-step=8600.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\lightning_logs\\version_3\\checkpoints\\epoch=9-step=8600.ckpt\n",
      "c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\dlf\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97700af845b410e84c58edfe854795d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9682000279426575     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9682000279426575    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\lightning_logs\\version_3\\checkpoints\\epoch=9-step=8600.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\lightning_logs\\version_3\\checkpoints\\epoch=9-step=8600.ckpt\n",
      "c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\dlf\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675d627d64cd40828f0f6839d16c801e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9749454259872437     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9749454259872437    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\lightning_logs\\version_3\\checkpoints\\epoch=9-step=8600.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at c:\\Users\\homeuser\\Documents\\deep_learning_fundamentals\\lightning_logs\\version_3\\checkpoints\\epoch=9-step=8600.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd80ae4a7de8402781a8faa3ea51189f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9667999744415283     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9667999744415283    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train accuracy: 97.49, \n",
      "Validation accuracy: 96.68, \n",
      "Test accuracy: 96.82\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model based on test_step which is computed after training\n",
    "test_acc = trainer.test(dataloaders=test_dl)[0][\"test_acc\"]\n",
    "\n",
    "# Can be done on the train and validation dataset as well\n",
    "train_acc = trainer.test(dataloaders=train_dl)[0][\"test_acc\"]\n",
    "val_acc = trainer.test(dataloaders=val_dl)[0][\"test_acc\"]\n",
    "\n",
    "print(f\"\\nTrain accuracy: {train_acc*100:.2f}, \\nValidation accuracy: {val_acc*100:.2f}, \\nTest accuracy: {test_acc*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving and loading models\n",
    "# PATH = \"lightning.pt\"\n",
    "# torch.save(pytorch_model.state_dict(), PATH)\n",
    "\n",
    "# # Load the model\n",
    "# model = PyTorchMLP(num_features = 28*28, num_classes = 10)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define a LightningModule that receives a PyTorch model as input\n",
    "# class LightningModel(L.LightningModule): # Define a class that inherits from L.LightningModule\n",
    "\n",
    "#     def __init__(self, model, learning_rate):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.model = model\n",
    "#         self.learning_rate = learning_rate\n",
    "\n",
    "#     def forward(self, x): \n",
    "#         return self.model(x)\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         features, true_labels = batch\n",
    "#         logits = self(features)\n",
    "#         loss = F.cross_entropy(logits, true_labels)\n",
    "#         self.log(\"train_loss\", loss)\n",
    "#         return loss # Return the loss which will be passed to the optimizer to zero out gradients, compute gradients, and update weights\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         features, true_labels = batch\n",
    "#         logits = self(features)\n",
    "#         loss = F.cross_entropy(logits, true_labels)\n",
    "#         self.log(\"val_loss\", loss, prog_bar=True)\n",
    "#         # Doesn't return anything because we don't need to compute gradients during validation\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.SGD(self.parameters(), lr = self.learning_rate)\n",
    "#         return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding data modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIGHTNINGDATAMODULE\n",
    "A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data:\n",
    "\n",
    "A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
    "\n",
    "Download / tokenize / process.\n",
    "\n",
    "Clean and (maybe) save to disk.\n",
    "\n",
    "Load inside Dataset.\n",
    "\n",
    "Apply transforms (rotate, tokenize, etc…).\n",
    "\n",
    "Wrap inside a DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a datamodule, the following methods are used to create train/val/test/predict dataloaders:\n",
    "\n",
    "`prepare_data()`: used to download, tokenize, etc… data (only called on 1 GPU/TPU in distributed).\n",
    "\n",
    "`setup`: used to do data operations that you might want to perform on every GPU E.g count number of classes, build vocabulary, perform train/val/test splits, apply transforms\n",
    "\n",
    "`train_dataloader()`: returns the train dataloader. This is the dataloader that the trainer `fit` method uses\n",
    "\n",
    "`val_dataloader()`: returns the val dataloader(s). This is the dataloader that the trainer `validate` and `fit` method uses\n",
    "\n",
    "`test_dataloader()`: returns the test dataloader(s). This is the dataloader that the trainer `test` method uses\n",
    "\n",
    "`predict_dataloader()`: returns the predict dataloader(s). This is the dataloader that the trainer `predict` method uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# Data modules as an optional organization layer\n",
    "class MNISTDataModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir = \"./mnist\", batch_size = 64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self): # Download the dataset - only called on 1 GPU\n",
    "        datasets.MNIST(self.data_dir, train = True, download = True)\n",
    "        datasets.MNIST(self.data_dir, train = False, download = True)\n",
    "\n",
    "    def setup(self, stage: str):# Split the dataset into training, validation, and test sets - called on every GPU\n",
    "        self.mnist_test = datasets.MNIST(\n",
    "            self.data_dir, transform = transforms.ToTensor(), train = False\n",
    "            )\n",
    "        mnist_full = datasets.MNIST(\n",
    "            self.data_dir, transform = transforms.ToTensor(), train = True\n",
    "        )\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        self.mnist_predict = datasets.MNIST(\n",
    "            self.data_dir, transform = transforms.ToTensor(), train = False\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_train, batch_size = self.batch_size, shuffle = True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_val, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_test, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_predict, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "\n",
    "# Create a Pytorch model\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.all_layers = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(num_features, 50),\n",
    "            nn.ReLU(),\n",
    "            # Second hidden layer\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            # Output layer\n",
    "            nn.Linear(25, num_classes)\n",
    "        )\n",
    "\n",
    "    # Foward the input through the model\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # Flatten all dimensions except batch\n",
    "        logits = self.all_layers(x)\n",
    "        return logits\n",
    "\n",
    "# Create a lightning module\n",
    "class LightningModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Save settings and hyperparameters to the log directory\n",
    "        # but don't save the model\n",
    "        self.save_hyperparameters(ignore = [\"model\"])\n",
    "\n",
    "        # Set up metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use forward pass of Pytorch model\n",
    "        return self.model(x)\n",
    "    \n",
    "    # Define shared step for training, validation, and test steps\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        predicted_labels = torch.argmax(logits, dim = 1)\n",
    "        return loss, predicted_labels, true_labels\n",
    "    \n",
    "    # Train loop\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\",\n",
    "            self.train_acc,\n",
    "            prog_bar = True,\n",
    "            on_epoch = True, on_step = False\n",
    "        )\n",
    "        return loss # # Return the loss which will be passed to the optimizer to zero out gradients, compute gradients, and update weights\n",
    "    \n",
    "    # Validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar = True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar = True)\n",
    "\n",
    "    # Test loop\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    # Define optimizers and learning rate schedulers\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr = self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "torch.manual_seed(2056)\n",
    "\n",
    "# Instantiate the data module\n",
    "dm = MNISTDataModule(data_dir = \"./mnist\", batch_size = 64)\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "pytorch_model = PyTorchMLP(num_features = 28*28, num_classes = 10)\n",
    "\n",
    "# Instantiate the Lightning model\n",
    "lightning_model = LightningModel(model = pytorch_model, learning_rate = 0.05)\n",
    "\n",
    "# Customize aspects of the training process using a Trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs = 10,\n",
    "    accelerator = \"auto\", # Set to auto to use GPU if available\n",
    "    devices = \"auto\", # Set to auto to use all GPUs if available\n",
    "    logger = CSVLogger(\"./logs\", name = \"mnist_logs\"),\n",
    "    #default_root_dir = \"./logs\", # Set the root directory for logs and weights while using tensorboard etc\n",
    "    #deterministic=True # Set to True to ensure reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(\n",
    "    model = lightning_model,\n",
    "    datamodule = dm\n",
    ")\n",
    "\n",
    "# Print how the model performs on the test dataset\n",
    "test_acc = trainer.test(dataloaders = dm.test_dataloader())[0][\"test_acc\"]\n",
    "\n",
    "# The same can be done on the train and validation dataset as well\n",
    "train_acc = trainer.test(dataloaders=dm.train_dataloader())[0][\"test_acc\"]\n",
    "val_acc = trainer.test(dataloaders=dm.val_dataloader())[0][\"test_acc\"]\n",
    "\n",
    "print(f\"\\nTrain accuracy: {train_acc*100:.2f}, \\nValidation accuracy: {val_acc*100:.2f}, \\nTest accuracy: {test_acc*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open TensorBoard\n",
    "# %load_ext tensorboard\n",
    "%tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all current logs\n",
    "import shutil\n",
    "shutil.rmtree(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning automatically logs checkpoints but you can manually log checkpoints as well\n",
    "#trainer.save_checkpoint(\"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# Data modules as an optional organization layer\n",
    "class MNISTDataModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir = \"./mnist\", batch_size = 64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self): # Download the dataset - only called on 1 GPU\n",
    "        datasets.MNIST(self.data_dir, train = True, download = True)\n",
    "        datasets.MNIST(self.data_dir, train = False, download = True)\n",
    "\n",
    "    def setup(self, stage: str):# Split the dataset into training, validation, and test sets - called on every GPU\n",
    "        self.mnist_test = datasets.MNIST(\n",
    "            self.data_dir, transform = transforms.ToTensor(), train = False\n",
    "            )\n",
    "        mnist_full = datasets.MNIST(\n",
    "            self.data_dir, transform = transforms.ToTensor(), train = True\n",
    "        )\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        self.mnist_predict = datasets.MNIST(\n",
    "            self.data_dir, transform = transforms.ToTensor(), train = False\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_train, batch_size = self.batch_size, shuffle = True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_val, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_test, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(dataset = self.mnist_predict, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "\n",
    "# Create a Pytorch model\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.all_layers = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(num_features, 50),\n",
    "            nn.ReLU(),\n",
    "            # Second hidden layer\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            # Output layer\n",
    "            nn.Linear(25, num_classes)\n",
    "        )\n",
    "\n",
    "    # Foward the input through the model\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # Flatten all dimensions except batch\n",
    "        logits = self.all_layers(x)\n",
    "        return logits\n",
    "\n",
    "# Create a lightning module\n",
    "class LightningModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Save settings and hyperparameters to the log directory\n",
    "        # but don't save the model\n",
    "        self.save_hyperparameters(ignore = [\"model\"])\n",
    "\n",
    "        # Set up metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use forward pass of Pytorch model\n",
    "        return self.model(x)\n",
    "    \n",
    "    # Define shared step for training, validation, and test steps\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        predicted_labels = torch.argmax(logits, dim = 1)\n",
    "        return loss, predicted_labels, true_labels\n",
    "    \n",
    "    # Train loop\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\",\n",
    "            self.train_acc,\n",
    "            prog_bar = True,\n",
    "            on_epoch = True, on_step = False\n",
    "        )\n",
    "        return loss # # Return the loss which will be passed to the optimizer to zero out gradients, compute gradients, and update weights\n",
    "    \n",
    "    # Validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar = True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar = True)\n",
    "\n",
    "    # Test loop\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, predicted_labels, true_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    # Define optimizers and learning rate schedulers\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr = self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.47\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pytorch model\n",
    "pytorch_model = PyTorchMLP(num_features = 28*28, num_classes = 10)\n",
    "\n",
    "# Instantiate the Lightning model from checkpoint\n",
    "lightning_model = LightningModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"model.ckpt\",\n",
    "    model = pytorch_model,\n",
    ")\n",
    "\n",
    "# Put the model in eval mode\n",
    "lightning_model = lightning_model.eval() # Can be done using pytorch_model.eval() as well\n",
    "\n",
    "# Instantiate the data module\n",
    "dm = MNISTDataModule(data_dir = \"./mnist\", batch_size = 64)\n",
    "dm.setup(stage = \"test\")\n",
    "\n",
    "# Define test dl that will pass the test dataset to the model in batches\n",
    "test_dl = dm.test_dataloader()\n",
    "acc = torchmetrics.Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "\n",
    "# Test loop\n",
    "with torch.inference_mode():\n",
    "    for batch in test_dl:\n",
    "        features, true_labels = batch\n",
    "        logits = lightning_model(features)\n",
    "        predicted_labels = torch.argmax(logits, dim = 1)\n",
    "        acc(predicted_labels, true_labels)\n",
    "\n",
    "# Compute accuracy\n",
    "test_acc = acc.compute()\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning data module: datasets, dataloaders\n",
    "Lightning module: model, training, validation, testing, prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
